# -*- coding: utf-8 -*-
"""IBM_one.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w0-qei6G0BIt-zomZRGlWjuSNAbDIReD
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import math
from nltk.tokenize import word_tokenize
from collections import defaultdict
from nltk.translate.metrics import alignment_error_rate
from nltk.metrics.scores import precision, recall,accuracy
from operator import itemgetter

#DATA
file = open('/content/drive/MyDrive/new/Data_Files/train.en', 'r')
train_en = file.read()
raw_sentences_train_en = train_en.split("\n")

file = open('/content/drive/MyDrive/new/Data_Files/train.hi', 'r')
train_hi = file.read()
raw_sentences_train_hi = train_hi.split("\n")

file = open('/content/drive/MyDrive/new/Data_Files/test.en', 'r')
test_en = file.read()
raw_sentences_test_en = test_en.split("\n")

file = open('/content/drive/MyDrive/new/Data_Files/test.hi', 'r')
test_hi = file.read()
raw_sentences_test_hi = test_hi.split("\n")

def add_null(data):
    list_ = []
    for sentence in data:
        list_.append("NULL " + sentence)
#     print(list_[2])
    return list_

sentences_test_hi = add_null(raw_sentences_test_hi)
sentences_train_hi = add_null(raw_sentences_train_hi)

def make_lower_case(data):
    list_ = []
    for sentence in data:
        list_.append("NULL " + sentence.lower())
#     print(list_[2])
    return list_

sentences_test_en = make_lower_case(raw_sentences_test_en)
sentences_train_en = make_lower_case(raw_sentences_train_en)

def is_converged(new, old, epoch):
    epsilone = 0.00000001
    if epoch < 15:
        return False
    return True

def perform_EM(en_sentences, hi_sentences):
    
    uni_ini = 0.00001
    
    translation_prob = defaultdict(lambda: float(uni_ini))
    translation_prob_prev = defaultdict(float)
    
    epoch = 0
    
    while True:

        epoch += 1
        print("epoch num:", epoch,"\n")
        count = defaultdict(float)
        total = defaultdict(float)
        s_total = defaultdict(float)
        
        for index_sen, hin_sen in enumerate(hi_sentences):
            #compute normalization
            hin_sen_words = hin_sen.split(" ")
            for hin_word in hin_sen_words:
                s_total[hin_word] = 0
                eng_sen_words = en_sentences[index_sen].split(" ")
                for eng_word in eng_sen_words:
                        s_total[hin_word] += translation_prob[(hin_word, eng_word)]
            
            #collect counts
            for hin_word in hin_sen_words:
                eng_sen_words = en_sentences[index_sen].split(" ")
                for eng_word in eng_sen_words:
                        count[(hin_word, eng_word)] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]
                        total[eng_word] += translation_prob[(hin_word, eng_word)]/s_total[hin_word]                   

        #estimate probabilities
        for (hin_word, eng_word) in translation_prob.keys():
                translation_prob[(hin_word, eng_word)] = count[(hin_word, eng_word)]/total[eng_word]

        if is_converged(translation_prob, translation_prob_prev, epoch) == True:
            break
        
        translation_prob_prev = translation_prob
        
        
    return translation_prob

def train_model(sentences_train_en, sentences_train_hi):
    
    translation_prob = perform_EM(sentences_train_en, sentences_train_hi)
    return translation_prob

print(len(sentences_train_hi),len(sentences_train_en),len(sentences_test_en),len(sentences_test_hi))

tef = train_model(sentences_train_en, sentences_train_hi)

tef



iterations = 0
for ((e_word, hin_word), value) in sorted(tef.items(), key=itemgetter(1), reverse=True):
    if iterations < 20:
        print("{:<20}{:>20.2}".format("P(%s|%s)" %(e_word, hin_word), value))
    else:
        break
    iterations += 1

def translate_sentence(sentence, tef, file):
    '''
    takes the best translation of an hindi word
    and appends to eng sentence
    '''
    eng_sentence = []
#     print("hin:",sentence)
    
    tokens = sentence.split(" ")
    for idx, token in enumerate(tokens):
        probabilities = []
        eng_words = []
#         print("token id, token", idx, token)
        max_score = -1
        max_eng_word = ""
        for k, v in tef.items():
            if token == k[0]:
                probabilities.append(v)
                eng_words.append(k[1])
#             print(f'{k[0]} - {v}')
#         probabilities = list(tef[token].values())
#         eng_words = list(tef[token].keys())
        for tef_index, prob in enumerate(probabilities):
            if prob > max_score:
                max_score = prob
                max_eng_word = eng_words[tef_index]
        
        eng_sentence.append(max_eng_word)
    
    eng_sentence = " ".join(eng_sentence)
#     print("eng:", eng_sentence)
    file.write(eng_sentence)
    file.write("\n")
    
    return eng_sentence

def test_model(dataset, tef):
    file = open("dev_translations.txt", 'w+')
    translated_data = []
    for sentence in dataset:
        translation = translate_sentence(sentence, tef, file)
        translated_data.append(translation)
    
    file.close()
    return translated_data

predicted_translations = test_model(sentences_test_en, tef)

def find_recall_precision_scores(predicted_translations):
    y_pred = set(list(predicted_translations))
    y_true = set(sentences_test_hi)
    recall_score = recall(y_true, y_pred)
    precision_score = precision(y_true, y_pred)
    return recall_score, precision_score

a,b=find_recall_precision_scores(predicted_translations)
print(a,b)