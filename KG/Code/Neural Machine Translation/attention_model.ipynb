{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXnDmXR7RDr2",
        "outputId": "21d40dac-eeee-4f49-ee58-643881cc6593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04m-jvKRDsJ",
        "outputId": "107697b9-117b-4db5-af97-40a97e7fac22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.690062036001109\n",
            "GPU (s):\n",
            "0.09407736399953137\n",
            "GPU speedup over CPU: 28x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp7ZGV-PVrjj"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wADhEIAVwcJ",
        "outputId": "1f7f6055-3076-4d9d-bea8-e64a9962b96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TARDkkIUaVcc",
        "outputId": "a4542950-306a-4f33-9301-a3f08100d1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "open('/content/drive/My Drive/en_hi_data.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/drive/My Drive/en_hi_data.zip' mode='r' encoding='UTF-8'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hzW8NEmeL0U"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaY8kXbpLTbL"
      },
      "source": [
        "data='en_hi.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX16sWiNZNf_"
      },
      "source": [
        "def create_dataset(path=data):\n",
        " lines=pd.read_pickle(path)\n",
        " lines=lines.dropna()\n",
        " #lines = lines[lines['source']=='ted']\n",
        " en = []\n",
        " hd = []\n",
        " for i, j in zip(lines['english'], lines['hindi']):\n",
        "  en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "  en_1.append('<end>')\n",
        "  en_1.insert(0, '<start>')\n",
        "  hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "  hd_1.append('<end>')\n",
        "  hd_1.insert(0, '<start>')\n",
        "  en.append(en_1)\n",
        "  hd.append(hd_1)\n",
        " return hd, en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeWjaz8PcPIb",
        "outputId": "ffc54144-d136-40f2-d8d7-9269282a9e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!unzip '/content/drive/My Drive/en_hi_data.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/en_hi_data.zip\n",
            "replace english_sampled.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace en_hi.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace hindi_sampled.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhhWUFlocD2m"
      },
      "source": [
        "PATH='en_hi.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hv2uuNUfKRk",
        "outputId": "46e7c634-e606-46c3-ab56-84da2df6d4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset('en_hi.pkl')\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.002)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9980 9980 20 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkrOtRulTS-W",
        "outputId": "dead64f8-66bd-4779-cbf7-94a59d0b2053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(input_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1    3  194 6985   24 2374    2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ezg2Tcf1nl",
        "outputId": "c2943788-5a30-4ec1-fc45-612050b7fb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_val[11])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_val[11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "3 ----> \n",
            "40 ----> no ,\n",
            "1572 ----> horse\n",
            "477 ----> shit .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "6 ----> -\n",
            "48 ----> नहीं,\n",
            "891 ----> घोड़े\n",
            "23 ----> की\n",
            "5160 ----> लीद.\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZD38bwcgAV7"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbh4PIGzgHfz"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP6RX-7kgKhb"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHo03zLEgO5N"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inFPx8CfgPvw"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-4qFsBBgR3E"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfeXBPAggWCn",
        "outputId": "2104bbdf-163d-40e0-ae56-6110f04b8719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 200\n",
        "train_loss =[]\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    \n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  loss = total_loss / steps_per_epoch\n",
        "  train_loss.append(loss)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      loss))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "xc= range(200)\n",
        "plt.figure()\n",
        "plt.plot(xc, train_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.2808\n",
            "Epoch 1 Batch 100 Loss 1.2188\n",
            "Epoch 1 Loss 1.2391\n",
            "Time taken for 1 epoch 12.519456148147583 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.1551\n",
            "Epoch 2 Batch 100 Loss 1.1077\n",
            "Epoch 2 Loss 1.1603\n",
            "Time taken for 1 epoch 12.417015075683594 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1458\n",
            "Epoch 3 Batch 100 Loss 1.0952\n",
            "Epoch 3 Loss 1.0872\n",
            "Time taken for 1 epoch 12.451569557189941 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0125\n",
            "Epoch 4 Batch 100 Loss 1.0579\n",
            "Epoch 4 Loss 1.0309\n",
            "Time taken for 1 epoch 12.568125009536743 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9256\n",
            "Epoch 5 Batch 100 Loss 1.0112\n",
            "Epoch 5 Loss 0.9818\n",
            "Time taken for 1 epoch 12.04902720451355 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9789\n",
            "Epoch 6 Batch 100 Loss 0.9032\n",
            "Epoch 6 Loss 0.9363\n",
            "Time taken for 1 epoch 12.525524616241455 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9224\n",
            "Epoch 7 Batch 100 Loss 0.8862\n",
            "Epoch 7 Loss 0.8944\n",
            "Time taken for 1 epoch 12.428106307983398 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8572\n",
            "Epoch 8 Batch 100 Loss 0.8306\n",
            "Epoch 8 Loss 0.8547\n",
            "Time taken for 1 epoch 12.409685611724854 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8408\n",
            "Epoch 9 Batch 100 Loss 0.8431\n",
            "Epoch 9 Loss 0.8133\n",
            "Time taken for 1 epoch 12.447327136993408 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.7680\n",
            "Epoch 10 Batch 100 Loss 0.7542\n",
            "Epoch 10 Loss 0.7706\n",
            "Time taken for 1 epoch 12.254540205001831 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6931\n",
            "Epoch 11 Batch 100 Loss 0.6892\n",
            "Epoch 11 Loss 0.7281\n",
            "Time taken for 1 epoch 12.327528476715088 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.7733\n",
            "Epoch 12 Batch 100 Loss 0.6936\n",
            "Epoch 12 Loss 0.6851\n",
            "Time taken for 1 epoch 12.235076665878296 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.6020\n",
            "Epoch 13 Batch 100 Loss 0.6515\n",
            "Epoch 13 Loss 0.6423\n",
            "Time taken for 1 epoch 12.457327604293823 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.5208\n",
            "Epoch 14 Batch 100 Loss 0.6051\n",
            "Epoch 14 Loss 0.6025\n",
            "Time taken for 1 epoch 12.207773447036743 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.6037\n",
            "Epoch 15 Batch 100 Loss 0.5269\n",
            "Epoch 15 Loss 0.5634\n",
            "Time taken for 1 epoch 12.348393678665161 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.5131\n",
            "Epoch 16 Batch 100 Loss 0.5032\n",
            "Epoch 16 Loss 0.5264\n",
            "Time taken for 1 epoch 12.383579730987549 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.4499\n",
            "Epoch 17 Batch 100 Loss 0.5479\n",
            "Epoch 17 Loss 0.4897\n",
            "Time taken for 1 epoch 12.388080596923828 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4625\n",
            "Epoch 18 Batch 100 Loss 0.5023\n",
            "Epoch 18 Loss 0.4562\n",
            "Time taken for 1 epoch 12.465024948120117 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.3813\n",
            "Epoch 19 Batch 100 Loss 0.3936\n",
            "Epoch 19 Loss 0.4233\n",
            "Time taken for 1 epoch 12.399399280548096 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3600\n",
            "Epoch 20 Batch 100 Loss 0.4237\n",
            "Epoch 20 Loss 0.3936\n",
            "Time taken for 1 epoch 12.421589136123657 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.3307\n",
            "Epoch 21 Batch 100 Loss 0.4223\n",
            "Epoch 21 Loss 0.3647\n",
            "Time taken for 1 epoch 12.455966711044312 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.3201\n",
            "Epoch 22 Batch 100 Loss 0.3260\n",
            "Epoch 22 Loss 0.3381\n",
            "Time taken for 1 epoch 12.438307762145996 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2989\n",
            "Epoch 23 Batch 100 Loss 0.3455\n",
            "Epoch 23 Loss 0.3143\n",
            "Time taken for 1 epoch 12.378156423568726 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.2906\n",
            "Epoch 24 Batch 100 Loss 0.2706\n",
            "Epoch 24 Loss 0.2921\n",
            "Time taken for 1 epoch 12.460127830505371 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.2741\n",
            "Epoch 25 Batch 100 Loss 0.2764\n",
            "Epoch 25 Loss 0.2715\n",
            "Time taken for 1 epoch 12.221237182617188 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.2642\n",
            "Epoch 26 Batch 100 Loss 0.2277\n",
            "Epoch 26 Loss 0.2544\n",
            "Time taken for 1 epoch 12.243304252624512 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.1885\n",
            "Epoch 27 Batch 100 Loss 0.2329\n",
            "Epoch 27 Loss 0.2358\n",
            "Time taken for 1 epoch 12.37555480003357 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.2116\n",
            "Epoch 28 Batch 100 Loss 0.2159\n",
            "Epoch 28 Loss 0.2179\n",
            "Time taken for 1 epoch 12.168380975723267 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.1853\n",
            "Epoch 29 Batch 100 Loss 0.1809\n",
            "Epoch 29 Loss 0.2027\n",
            "Time taken for 1 epoch 12.44268250465393 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.2095\n",
            "Epoch 30 Batch 100 Loss 0.1830\n",
            "Epoch 30 Loss 0.1889\n",
            "Time taken for 1 epoch 12.605185747146606 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.1992\n",
            "Epoch 31 Batch 100 Loss 0.1562\n",
            "Epoch 31 Loss 0.1755\n",
            "Time taken for 1 epoch 12.324387073516846 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.1545\n",
            "Epoch 32 Batch 100 Loss 0.1554\n",
            "Epoch 32 Loss 0.1630\n",
            "Time taken for 1 epoch 12.506048202514648 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.1375\n",
            "Epoch 33 Batch 100 Loss 0.1448\n",
            "Epoch 33 Loss 0.1518\n",
            "Time taken for 1 epoch 12.051126956939697 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.1223\n",
            "Epoch 34 Batch 100 Loss 0.1523\n",
            "Epoch 34 Loss 0.1408\n",
            "Time taken for 1 epoch 12.275094270706177 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.1034\n",
            "Epoch 35 Batch 100 Loss 0.1298\n",
            "Epoch 35 Loss 0.1312\n",
            "Time taken for 1 epoch 12.39026951789856 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.1014\n",
            "Epoch 36 Batch 100 Loss 0.1170\n",
            "Epoch 36 Loss 0.1214\n",
            "Time taken for 1 epoch 12.325912952423096 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0938\n",
            "Epoch 37 Batch 100 Loss 0.1122\n",
            "Epoch 37 Loss 0.1118\n",
            "Time taken for 1 epoch 12.31696367263794 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.1023\n",
            "Epoch 38 Batch 100 Loss 0.0874\n",
            "Epoch 38 Loss 0.1052\n",
            "Time taken for 1 epoch 12.529873132705688 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0914\n",
            "Epoch 39 Batch 100 Loss 0.0999\n",
            "Epoch 39 Loss 0.0975\n",
            "Time taken for 1 epoch 12.491829872131348 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0709\n",
            "Epoch 40 Batch 100 Loss 0.0933\n",
            "Epoch 40 Loss 0.0894\n",
            "Time taken for 1 epoch 12.330248594284058 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0664\n",
            "Epoch 41 Batch 100 Loss 0.0947\n",
            "Epoch 41 Loss 0.0827\n",
            "Time taken for 1 epoch 12.473809719085693 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0798\n",
            "Epoch 42 Batch 100 Loss 0.0964\n",
            "Epoch 42 Loss 0.0786\n",
            "Time taken for 1 epoch 12.202930450439453 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0533\n",
            "Epoch 43 Batch 100 Loss 0.0659\n",
            "Epoch 43 Loss 0.0731\n",
            "Time taken for 1 epoch 12.47810411453247 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0613\n",
            "Epoch 44 Batch 100 Loss 0.0690\n",
            "Epoch 44 Loss 0.0680\n",
            "Time taken for 1 epoch 12.46458911895752 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0621\n",
            "Epoch 45 Batch 100 Loss 0.0654\n",
            "Epoch 45 Loss 0.0635\n",
            "Time taken for 1 epoch 12.522132873535156 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0554\n",
            "Epoch 46 Batch 100 Loss 0.0518\n",
            "Epoch 46 Loss 0.0587\n",
            "Time taken for 1 epoch 12.345300674438477 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0405\n",
            "Epoch 47 Batch 100 Loss 0.0532\n",
            "Epoch 47 Loss 0.0558\n",
            "Time taken for 1 epoch 12.422684669494629 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0487\n",
            "Epoch 48 Batch 100 Loss 0.0598\n",
            "Epoch 48 Loss 0.0510\n",
            "Time taken for 1 epoch 12.407191038131714 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0504\n",
            "Epoch 49 Batch 100 Loss 0.0517\n",
            "Epoch 49 Loss 0.0464\n",
            "Time taken for 1 epoch 12.25516939163208 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0372\n",
            "Epoch 50 Batch 100 Loss 0.0353\n",
            "Epoch 50 Loss 0.0436\n",
            "Time taken for 1 epoch 12.412826299667358 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0374\n",
            "Epoch 51 Batch 100 Loss 0.0427\n",
            "Epoch 51 Loss 0.0416\n",
            "Time taken for 1 epoch 12.172765493392944 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0274\n",
            "Epoch 52 Batch 100 Loss 0.0419\n",
            "Epoch 52 Loss 0.0390\n",
            "Time taken for 1 epoch 12.381707191467285 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0423\n",
            "Epoch 53 Batch 100 Loss 0.0312\n",
            "Epoch 53 Loss 0.0382\n",
            "Time taken for 1 epoch 12.419928550720215 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0286\n",
            "Epoch 54 Batch 100 Loss 0.0281\n",
            "Epoch 54 Loss 0.0354\n",
            "Time taken for 1 epoch 12.441628694534302 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0232\n",
            "Epoch 55 Batch 100 Loss 0.0392\n",
            "Epoch 55 Loss 0.0326\n",
            "Time taken for 1 epoch 12.298959732055664 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0370\n",
            "Epoch 56 Batch 100 Loss 0.0307\n",
            "Epoch 56 Loss 0.0300\n",
            "Time taken for 1 epoch 12.597176313400269 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0213\n",
            "Epoch 57 Batch 100 Loss 0.0254\n",
            "Epoch 57 Loss 0.0272\n",
            "Time taken for 1 epoch 12.26926589012146 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0245\n",
            "Epoch 58 Batch 100 Loss 0.0302\n",
            "Epoch 58 Loss 0.0247\n",
            "Time taken for 1 epoch 12.389051914215088 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0240\n",
            "Epoch 59 Batch 100 Loss 0.0257\n",
            "Epoch 59 Loss 0.0243\n",
            "Time taken for 1 epoch 12.60359501838684 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0199\n",
            "Epoch 60 Batch 100 Loss 0.0302\n",
            "Epoch 60 Loss 0.0246\n",
            "Time taken for 1 epoch 12.618764638900757 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0152\n",
            "Epoch 61 Batch 100 Loss 0.0248\n",
            "Epoch 61 Loss 0.0239\n",
            "Time taken for 1 epoch 12.561450242996216 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0327\n",
            "Epoch 62 Batch 100 Loss 0.0254\n",
            "Epoch 62 Loss 0.0257\n",
            "Time taken for 1 epoch 12.583424806594849 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0177\n",
            "Epoch 63 Batch 100 Loss 0.0214\n",
            "Epoch 63 Loss 0.0263\n",
            "Time taken for 1 epoch 12.457963466644287 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0189\n",
            "Epoch 64 Batch 100 Loss 0.0335\n",
            "Epoch 64 Loss 0.0255\n",
            "Time taken for 1 epoch 12.637280225753784 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0280\n",
            "Epoch 65 Batch 100 Loss 0.0184\n",
            "Epoch 65 Loss 0.0213\n",
            "Time taken for 1 epoch 12.532206773757935 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0187\n",
            "Epoch 66 Batch 100 Loss 0.0286\n",
            "Epoch 66 Loss 0.0257\n",
            "Time taken for 1 epoch 12.738647937774658 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0185\n",
            "Epoch 67 Batch 100 Loss 0.0199\n",
            "Epoch 67 Loss 0.0228\n",
            "Time taken for 1 epoch 12.825247764587402 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0270\n",
            "Epoch 68 Batch 100 Loss 0.0273\n",
            "Epoch 68 Loss 0.0189\n",
            "Time taken for 1 epoch 12.739408731460571 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0170\n",
            "Epoch 69 Batch 100 Loss 0.0192\n",
            "Epoch 69 Loss 0.0152\n",
            "Time taken for 1 epoch 12.474213361740112 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0127\n",
            "Epoch 70 Batch 100 Loss 0.0174\n",
            "Epoch 70 Loss 0.0135\n",
            "Time taken for 1 epoch 12.864395141601562 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0063\n",
            "Epoch 71 Batch 100 Loss 0.0084\n",
            "Epoch 71 Loss 0.0120\n",
            "Time taken for 1 epoch 12.753400564193726 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0092\n",
            "Epoch 72 Batch 100 Loss 0.0100\n",
            "Epoch 72 Loss 0.0115\n",
            "Time taken for 1 epoch 12.686653137207031 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0131\n",
            "Epoch 73 Batch 100 Loss 0.0081\n",
            "Epoch 73 Loss 0.0119\n",
            "Time taken for 1 epoch 12.54947543144226 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0276\n",
            "Epoch 74 Batch 100 Loss 0.0082\n",
            "Epoch 74 Loss 0.0124\n",
            "Time taken for 1 epoch 12.478208541870117 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0071\n",
            "Epoch 75 Batch 100 Loss 0.0116\n",
            "Epoch 75 Loss 0.0117\n",
            "Time taken for 1 epoch 12.573848962783813 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0072\n",
            "Epoch 76 Batch 100 Loss 0.0061\n",
            "Epoch 76 Loss 0.0114\n",
            "Time taken for 1 epoch 12.573206901550293 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0055\n",
            "Epoch 77 Batch 100 Loss 0.0109\n",
            "Epoch 77 Loss 0.0109\n",
            "Time taken for 1 epoch 12.48490571975708 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0132\n",
            "Epoch 78 Batch 100 Loss 0.0083\n",
            "Epoch 78 Loss 0.0140\n",
            "Time taken for 1 epoch 12.14932632446289 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0155\n",
            "Epoch 79 Batch 100 Loss 0.0240\n",
            "Epoch 79 Loss 0.0234\n",
            "Time taken for 1 epoch 12.30232834815979 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0248\n",
            "Epoch 80 Batch 100 Loss 0.0192\n",
            "Epoch 80 Loss 0.0256\n",
            "Time taken for 1 epoch 12.508289813995361 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0149\n",
            "Epoch 81 Batch 100 Loss 0.0206\n",
            "Epoch 81 Loss 0.0208\n",
            "Time taken for 1 epoch 12.368686199188232 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0101\n",
            "Epoch 82 Batch 100 Loss 0.0184\n",
            "Epoch 82 Loss 0.0141\n",
            "Time taken for 1 epoch 12.525497198104858 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0101\n",
            "Epoch 83 Batch 100 Loss 0.0109\n",
            "Epoch 83 Loss 0.0118\n",
            "Time taken for 1 epoch 12.348339319229126 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0066\n",
            "Epoch 84 Batch 100 Loss 0.0090\n",
            "Epoch 84 Loss 0.0100\n",
            "Time taken for 1 epoch 12.439188957214355 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0087\n",
            "Epoch 85 Batch 100 Loss 0.0084\n",
            "Epoch 85 Loss 0.0083\n",
            "Time taken for 1 epoch 12.330910921096802 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0040\n",
            "Epoch 86 Batch 100 Loss 0.0107\n",
            "Epoch 86 Loss 0.0072\n",
            "Time taken for 1 epoch 12.489448547363281 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0057\n",
            "Epoch 87 Batch 100 Loss 0.0091\n",
            "Epoch 87 Loss 0.0067\n",
            "Time taken for 1 epoch 12.33320951461792 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0045\n",
            "Epoch 88 Batch 100 Loss 0.0073\n",
            "Epoch 88 Loss 0.0068\n",
            "Time taken for 1 epoch 12.26159143447876 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0040\n",
            "Epoch 89 Batch 100 Loss 0.0100\n",
            "Epoch 89 Loss 0.0068\n",
            "Time taken for 1 epoch 12.835834741592407 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0051\n",
            "Epoch 90 Batch 100 Loss 0.0087\n",
            "Epoch 90 Loss 0.0067\n",
            "Time taken for 1 epoch 12.59647822380066 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0109\n",
            "Epoch 91 Batch 100 Loss 0.0069\n",
            "Epoch 91 Loss 0.0072\n",
            "Time taken for 1 epoch 12.386760711669922 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0126\n",
            "Epoch 92 Batch 100 Loss 0.0031\n",
            "Epoch 92 Loss 0.0069\n",
            "Time taken for 1 epoch 12.492174625396729 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0090\n",
            "Epoch 93 Batch 100 Loss 0.0119\n",
            "Epoch 93 Loss 0.0072\n",
            "Time taken for 1 epoch 12.420598268508911 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0073\n",
            "Epoch 94 Batch 100 Loss 0.0067\n",
            "Epoch 94 Loss 0.0080\n",
            "Time taken for 1 epoch 12.503848314285278 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0094\n",
            "Epoch 95 Batch 100 Loss 0.0209\n",
            "Epoch 95 Loss 0.0159\n",
            "Time taken for 1 epoch 12.286175012588501 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0143\n",
            "Epoch 96 Batch 100 Loss 0.0359\n",
            "Epoch 96 Loss 0.0241\n",
            "Time taken for 1 epoch 12.360074520111084 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0251\n",
            "Epoch 97 Batch 100 Loss 0.0312\n",
            "Epoch 97 Loss 0.0235\n",
            "Time taken for 1 epoch 12.329877853393555 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0122\n",
            "Epoch 98 Batch 100 Loss 0.0096\n",
            "Epoch 98 Loss 0.0143\n",
            "Time taken for 1 epoch 12.318976640701294 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0071\n",
            "Epoch 99 Batch 100 Loss 0.0115\n",
            "Epoch 99 Loss 0.0092\n",
            "Time taken for 1 epoch 12.281391143798828 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0080\n",
            "Epoch 100 Batch 100 Loss 0.0152\n",
            "Epoch 100 Loss 0.0074\n",
            "Time taken for 1 epoch 12.438909769058228 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0038\n",
            "Epoch 101 Batch 100 Loss 0.0061\n",
            "Epoch 101 Loss 0.0060\n",
            "Time taken for 1 epoch 12.492077350616455 sec\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0067\n",
            "Epoch 102 Batch 100 Loss 0.0049\n",
            "Epoch 102 Loss 0.0052\n",
            "Time taken for 1 epoch 12.166666507720947 sec\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0052\n",
            "Epoch 103 Batch 100 Loss 0.0034\n",
            "Epoch 103 Loss 0.0049\n",
            "Time taken for 1 epoch 12.340303659439087 sec\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.0035\n",
            "Epoch 104 Batch 100 Loss 0.0062\n",
            "Epoch 104 Loss 0.0047\n",
            "Time taken for 1 epoch 12.356906652450562 sec\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.0049\n",
            "Epoch 105 Batch 100 Loss 0.0033\n",
            "Epoch 105 Loss 0.0046\n",
            "Time taken for 1 epoch 12.162961483001709 sec\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.0016\n",
            "Epoch 106 Batch 100 Loss 0.0036\n",
            "Epoch 106 Loss 0.0045\n",
            "Time taken for 1 epoch 12.691206693649292 sec\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.0038\n",
            "Epoch 107 Batch 100 Loss 0.0035\n",
            "Epoch 107 Loss 0.0053\n",
            "Time taken for 1 epoch 12.12959337234497 sec\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.0068\n",
            "Epoch 108 Batch 100 Loss 0.0023\n",
            "Epoch 108 Loss 0.0053\n",
            "Time taken for 1 epoch 12.374835014343262 sec\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.0028\n",
            "Epoch 109 Batch 100 Loss 0.0079\n",
            "Epoch 109 Loss 0.0055\n",
            "Time taken for 1 epoch 12.637706756591797 sec\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0043\n",
            "Epoch 110 Batch 100 Loss 0.0069\n",
            "Epoch 110 Loss 0.0057\n",
            "Time taken for 1 epoch 12.24075436592102 sec\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0027\n",
            "Epoch 111 Batch 100 Loss 0.0066\n",
            "Epoch 111 Loss 0.0068\n",
            "Time taken for 1 epoch 12.238579988479614 sec\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.0065\n",
            "Epoch 112 Batch 100 Loss 0.0080\n",
            "Epoch 112 Loss 0.0085\n",
            "Time taken for 1 epoch 12.190465688705444 sec\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0071\n",
            "Epoch 113 Batch 100 Loss 0.0334\n",
            "Epoch 113 Loss 0.0207\n",
            "Time taken for 1 epoch 12.40781569480896 sec\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0148\n",
            "Epoch 114 Batch 100 Loss 0.0295\n",
            "Epoch 114 Loss 0.0254\n",
            "Time taken for 1 epoch 12.465511560440063 sec\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0125\n",
            "Epoch 115 Batch 100 Loss 0.0169\n",
            "Epoch 115 Loss 0.0163\n",
            "Time taken for 1 epoch 12.487436532974243 sec\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0089\n",
            "Epoch 116 Batch 100 Loss 0.0068\n",
            "Epoch 116 Loss 0.0101\n",
            "Time taken for 1 epoch 12.305426359176636 sec\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.0047\n",
            "Epoch 117 Batch 100 Loss 0.0068\n",
            "Epoch 117 Loss 0.0063\n",
            "Time taken for 1 epoch 12.649620056152344 sec\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.0044\n",
            "Epoch 118 Batch 100 Loss 0.0047\n",
            "Epoch 118 Loss 0.0051\n",
            "Time taken for 1 epoch 12.58375334739685 sec\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0054\n",
            "Epoch 119 Batch 100 Loss 0.0017\n",
            "Epoch 119 Loss 0.0044\n",
            "Time taken for 1 epoch 12.632062435150146 sec\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0051\n",
            "Epoch 120 Batch 100 Loss 0.0021\n",
            "Epoch 120 Loss 0.0040\n",
            "Time taken for 1 epoch 12.696765899658203 sec\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.0020\n",
            "Epoch 121 Batch 100 Loss 0.0032\n",
            "Epoch 121 Loss 0.0040\n",
            "Time taken for 1 epoch 12.452837228775024 sec\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.0019\n",
            "Epoch 122 Batch 100 Loss 0.0074\n",
            "Epoch 122 Loss 0.0041\n",
            "Time taken for 1 epoch 12.662088394165039 sec\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.0041\n",
            "Epoch 123 Batch 100 Loss 0.0023\n",
            "Epoch 123 Loss 0.0044\n",
            "Time taken for 1 epoch 12.299676895141602 sec\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.0025\n",
            "Epoch 124 Batch 100 Loss 0.0052\n",
            "Epoch 124 Loss 0.0043\n",
            "Time taken for 1 epoch 12.415166139602661 sec\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.0021\n",
            "Epoch 125 Batch 100 Loss 0.0030\n",
            "Epoch 125 Loss 0.0041\n",
            "Time taken for 1 epoch 12.574888229370117 sec\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.0045\n",
            "Epoch 126 Batch 100 Loss 0.0018\n",
            "Epoch 126 Loss 0.0044\n",
            "Time taken for 1 epoch 12.657289743423462 sec\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0028\n",
            "Epoch 127 Batch 100 Loss 0.0043\n",
            "Epoch 127 Loss 0.0040\n",
            "Time taken for 1 epoch 12.317723989486694 sec\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.0034\n",
            "Epoch 128 Batch 100 Loss 0.0060\n",
            "Epoch 128 Loss 0.0041\n",
            "Time taken for 1 epoch 12.260324478149414 sec\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0028\n",
            "Epoch 129 Batch 100 Loss 0.0042\n",
            "Epoch 129 Loss 0.0049\n",
            "Time taken for 1 epoch 12.40552043914795 sec\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.0059\n",
            "Epoch 130 Batch 100 Loss 0.0136\n",
            "Epoch 130 Loss 0.0097\n",
            "Time taken for 1 epoch 12.428358793258667 sec\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0078\n",
            "Epoch 131 Batch 100 Loss 0.0189\n",
            "Epoch 131 Loss 0.0220\n",
            "Time taken for 1 epoch 12.355051755905151 sec\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0224\n",
            "Epoch 132 Batch 100 Loss 0.0128\n",
            "Epoch 132 Loss 0.0205\n",
            "Time taken for 1 epoch 12.171406030654907 sec\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0168\n",
            "Epoch 133 Batch 100 Loss 0.0257\n",
            "Epoch 133 Loss 0.0121\n",
            "Time taken for 1 epoch 12.422546148300171 sec\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0054\n",
            "Epoch 134 Batch 100 Loss 0.0039\n",
            "Epoch 134 Loss 0.0077\n",
            "Time taken for 1 epoch 12.519283533096313 sec\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0052\n",
            "Epoch 135 Batch 100 Loss 0.0068\n",
            "Epoch 135 Loss 0.0057\n",
            "Time taken for 1 epoch 12.103792428970337 sec\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0040\n",
            "Epoch 136 Batch 100 Loss 0.0045\n",
            "Epoch 136 Loss 0.0047\n",
            "Time taken for 1 epoch 12.109266757965088 sec\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0031\n",
            "Epoch 137 Batch 100 Loss 0.0055\n",
            "Epoch 137 Loss 0.0042\n",
            "Time taken for 1 epoch 12.30609130859375 sec\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0040\n",
            "Epoch 138 Batch 100 Loss 0.0045\n",
            "Epoch 138 Loss 0.0037\n",
            "Time taken for 1 epoch 12.250755071640015 sec\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.0008\n",
            "Epoch 139 Batch 100 Loss 0.0020\n",
            "Epoch 139 Loss 0.0036\n",
            "Time taken for 1 epoch 12.550748109817505 sec\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0032\n",
            "Epoch 140 Batch 100 Loss 0.0081\n",
            "Epoch 140 Loss 0.0035\n",
            "Time taken for 1 epoch 12.324519634246826 sec\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0016\n",
            "Epoch 141 Batch 100 Loss 0.0014\n",
            "Epoch 141 Loss 0.0034\n",
            "Time taken for 1 epoch 12.258163452148438 sec\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0015\n",
            "Epoch 142 Batch 100 Loss 0.0017\n",
            "Epoch 142 Loss 0.0034\n",
            "Time taken for 1 epoch 12.340564727783203 sec\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0027\n",
            "Epoch 143 Batch 100 Loss 0.0076\n",
            "Epoch 143 Loss 0.0036\n",
            "Time taken for 1 epoch 12.58838677406311 sec\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.0035\n",
            "Epoch 144 Batch 100 Loss 0.0020\n",
            "Epoch 144 Loss 0.0041\n",
            "Time taken for 1 epoch 12.22464108467102 sec\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0028\n",
            "Epoch 145 Batch 100 Loss 0.0038\n",
            "Epoch 145 Loss 0.0041\n",
            "Time taken for 1 epoch 12.283233880996704 sec\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0039\n",
            "Epoch 146 Batch 100 Loss 0.0045\n",
            "Epoch 146 Loss 0.0046\n",
            "Time taken for 1 epoch 12.264813423156738 sec\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0025\n",
            "Epoch 147 Batch 100 Loss 0.0132\n",
            "Epoch 147 Loss 0.0060\n",
            "Time taken for 1 epoch 12.382177352905273 sec\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0021\n",
            "Epoch 148 Batch 100 Loss 0.0176\n",
            "Epoch 148 Loss 0.0138\n",
            "Time taken for 1 epoch 12.344594717025757 sec\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0207\n",
            "Epoch 149 Batch 100 Loss 0.0186\n",
            "Epoch 149 Loss 0.0210\n",
            "Time taken for 1 epoch 12.347479343414307 sec\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0149\n",
            "Epoch 150 Batch 100 Loss 0.0121\n",
            "Epoch 150 Loss 0.0149\n",
            "Time taken for 1 epoch 12.426631927490234 sec\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0091\n",
            "Epoch 151 Batch 100 Loss 0.0095\n",
            "Epoch 151 Loss 0.0082\n",
            "Time taken for 1 epoch 12.45141339302063 sec\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.0026\n",
            "Epoch 152 Batch 100 Loss 0.0020\n",
            "Epoch 152 Loss 0.0057\n",
            "Time taken for 1 epoch 12.445354700088501 sec\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0036\n",
            "Epoch 153 Batch 100 Loss 0.0094\n",
            "Epoch 153 Loss 0.0045\n",
            "Time taken for 1 epoch 12.31695556640625 sec\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.0039\n",
            "Epoch 154 Batch 100 Loss 0.0051\n",
            "Epoch 154 Loss 0.0039\n",
            "Time taken for 1 epoch 12.428365707397461 sec\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.0022\n",
            "Epoch 155 Batch 100 Loss 0.0058\n",
            "Epoch 155 Loss 0.0036\n",
            "Time taken for 1 epoch 12.402782440185547 sec\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.0016\n",
            "Epoch 156 Batch 100 Loss 0.0039\n",
            "Epoch 156 Loss 0.0033\n",
            "Time taken for 1 epoch 12.415170192718506 sec\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.0048\n",
            "Epoch 157 Batch 100 Loss 0.0031\n",
            "Epoch 157 Loss 0.0032\n",
            "Time taken for 1 epoch 12.52271294593811 sec\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.0030\n",
            "Epoch 158 Batch 100 Loss 0.0005\n",
            "Epoch 158 Loss 0.0033\n",
            "Time taken for 1 epoch 12.541285991668701 sec\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.0032\n",
            "Epoch 159 Batch 100 Loss 0.0017\n",
            "Epoch 159 Loss 0.0033\n",
            "Time taken for 1 epoch 12.45812702178955 sec\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.0025\n",
            "Epoch 160 Batch 100 Loss 0.0042\n",
            "Epoch 160 Loss 0.0032\n",
            "Time taken for 1 epoch 12.682852745056152 sec\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0007\n",
            "Epoch 161 Batch 100 Loss 0.0017\n",
            "Epoch 161 Loss 0.0033\n",
            "Time taken for 1 epoch 12.57547640800476 sec\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.0004\n",
            "Epoch 162 Batch 100 Loss 0.0073\n",
            "Epoch 162 Loss 0.0036\n",
            "Time taken for 1 epoch 12.493833065032959 sec\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0057\n",
            "Epoch 163 Batch 100 Loss 0.0020\n",
            "Epoch 163 Loss 0.0037\n",
            "Time taken for 1 epoch 12.636985540390015 sec\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.0055\n",
            "Epoch 164 Batch 100 Loss 0.0033\n",
            "Epoch 164 Loss 0.0037\n",
            "Time taken for 1 epoch 12.703397750854492 sec\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0025\n",
            "Epoch 165 Batch 100 Loss 0.0046\n",
            "Epoch 165 Loss 0.0061\n",
            "Time taken for 1 epoch 12.476152658462524 sec\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.0052\n",
            "Epoch 166 Batch 100 Loss 0.0126\n",
            "Epoch 166 Loss 0.0128\n",
            "Time taken for 1 epoch 12.706387281417847 sec\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0098\n",
            "Epoch 167 Batch 100 Loss 0.0221\n",
            "Epoch 167 Loss 0.0175\n",
            "Time taken for 1 epoch 12.816359519958496 sec\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.0077\n",
            "Epoch 168 Batch 100 Loss 0.0135\n",
            "Epoch 168 Loss 0.0126\n",
            "Time taken for 1 epoch 12.48668622970581 sec\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0071\n",
            "Epoch 169 Batch 100 Loss 0.0074\n",
            "Epoch 169 Loss 0.0079\n",
            "Time taken for 1 epoch 12.43661642074585 sec\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.0060\n",
            "Epoch 170 Batch 100 Loss 0.0015\n",
            "Epoch 170 Loss 0.0054\n",
            "Time taken for 1 epoch 12.204474687576294 sec\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.0012\n",
            "Epoch 171 Batch 100 Loss 0.0032\n",
            "Epoch 171 Loss 0.0043\n",
            "Time taken for 1 epoch 12.40577483177185 sec\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.0015\n",
            "Epoch 172 Batch 100 Loss 0.0033\n",
            "Epoch 172 Loss 0.0038\n",
            "Time taken for 1 epoch 12.681923389434814 sec\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0010\n",
            "Epoch 173 Batch 100 Loss 0.0075\n",
            "Epoch 173 Loss 0.0036\n",
            "Time taken for 1 epoch 12.131552696228027 sec\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.0046\n",
            "Epoch 174 Batch 100 Loss 0.0052\n",
            "Epoch 174 Loss 0.0035\n",
            "Time taken for 1 epoch 12.383015394210815 sec\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.0012\n",
            "Epoch 175 Batch 100 Loss 0.0038\n",
            "Epoch 175 Loss 0.0033\n",
            "Time taken for 1 epoch 12.27750825881958 sec\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.0014\n",
            "Epoch 176 Batch 100 Loss 0.0030\n",
            "Epoch 176 Loss 0.0032\n",
            "Time taken for 1 epoch 12.235668182373047 sec\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0007\n",
            "Epoch 177 Batch 100 Loss 0.0016\n",
            "Epoch 177 Loss 0.0032\n",
            "Time taken for 1 epoch 12.468575239181519 sec\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0022\n",
            "Epoch 178 Batch 100 Loss 0.0073\n",
            "Epoch 178 Loss 0.0032\n",
            "Time taken for 1 epoch 12.577563285827637 sec\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0037\n",
            "Epoch 179 Batch 100 Loss 0.0004\n",
            "Epoch 179 Loss 0.0032\n",
            "Time taken for 1 epoch 12.494678974151611 sec\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.0017\n",
            "Epoch 180 Batch 100 Loss 0.0048\n",
            "Epoch 180 Loss 0.0032\n",
            "Time taken for 1 epoch 12.530024766921997 sec\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0060\n",
            "Epoch 181 Batch 100 Loss 0.0024\n",
            "Epoch 181 Loss 0.0033\n",
            "Time taken for 1 epoch 12.192681074142456 sec\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0023\n",
            "Epoch 182 Batch 100 Loss 0.0062\n",
            "Epoch 182 Loss 0.0042\n",
            "Time taken for 1 epoch 12.637832403182983 sec\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0049\n",
            "Epoch 183 Batch 100 Loss 0.0045\n",
            "Epoch 183 Loss 0.0075\n",
            "Time taken for 1 epoch 12.440092086791992 sec\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0287\n",
            "Epoch 184 Batch 100 Loss 0.0271\n",
            "Epoch 184 Loss 0.0218\n",
            "Time taken for 1 epoch 12.35919737815857 sec\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0086\n",
            "Epoch 185 Batch 100 Loss 0.0140\n",
            "Epoch 185 Loss 0.0141\n",
            "Time taken for 1 epoch 12.095775842666626 sec\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0059\n",
            "Epoch 186 Batch 100 Loss 0.0093\n",
            "Epoch 186 Loss 0.0082\n",
            "Time taken for 1 epoch 12.373233795166016 sec\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0031\n",
            "Epoch 187 Batch 100 Loss 0.0014\n",
            "Epoch 187 Loss 0.0056\n",
            "Time taken for 1 epoch 12.145905017852783 sec\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0045\n",
            "Epoch 188 Batch 100 Loss 0.0023\n",
            "Epoch 188 Loss 0.0040\n",
            "Time taken for 1 epoch 12.4702467918396 sec\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0070\n",
            "Epoch 189 Batch 100 Loss 0.0049\n",
            "Epoch 189 Loss 0.0035\n",
            "Time taken for 1 epoch 12.693047761917114 sec\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0025\n",
            "Epoch 190 Batch 100 Loss 0.0015\n",
            "Epoch 190 Loss 0.0033\n",
            "Time taken for 1 epoch 12.438836574554443 sec\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0040\n",
            "Epoch 191 Batch 100 Loss 0.0036\n",
            "Epoch 191 Loss 0.0031\n",
            "Time taken for 1 epoch 12.517030715942383 sec\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0040\n",
            "Epoch 192 Batch 100 Loss 0.0024\n",
            "Epoch 192 Loss 0.0031\n",
            "Time taken for 1 epoch 12.483784198760986 sec\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0016\n",
            "Epoch 193 Batch 100 Loss 0.0029\n",
            "Epoch 193 Loss 0.0031\n",
            "Time taken for 1 epoch 12.488703966140747 sec\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0047\n",
            "Epoch 194 Batch 100 Loss 0.0039\n",
            "Epoch 194 Loss 0.0030\n",
            "Time taken for 1 epoch 12.493278503417969 sec\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0011\n",
            "Epoch 195 Batch 100 Loss 0.0009\n",
            "Epoch 195 Loss 0.0030\n",
            "Time taken for 1 epoch 12.351560354232788 sec\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0011\n",
            "Epoch 196 Batch 100 Loss 0.0037\n",
            "Epoch 196 Loss 0.0030\n",
            "Time taken for 1 epoch 12.697999000549316 sec\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0034\n",
            "Epoch 197 Batch 100 Loss 0.0017\n",
            "Epoch 197 Loss 0.0031\n",
            "Time taken for 1 epoch 12.494102478027344 sec\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0025\n",
            "Epoch 198 Batch 100 Loss 0.0097\n",
            "Epoch 198 Loss 0.0033\n",
            "Time taken for 1 epoch 12.218161344528198 sec\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0051\n",
            "Epoch 199 Batch 100 Loss 0.0165\n",
            "Epoch 199 Loss 0.0067\n",
            "Time taken for 1 epoch 12.618569135665894 sec\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0303\n",
            "Epoch 200 Batch 100 Loss 0.0134\n",
            "Epoch 200 Loss 0.0179\n",
            "Time taken for 1 epoch 12.525520086288452 sec\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f903cc60e10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmNikqrthRLO"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    #sentence = preprocess_sentence(sentence)\n",
        "    #inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([sentence],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Zrv-tVhiEN"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Fg_y5lgWCh"
      },
      "source": [
        "def convert2(lang, tensor):\n",
        "  hin_words=[]\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      \n",
        "      hin_words.append(lang.index_word[t])\n",
        "      \n",
        "  hin_words=hin_words[1:len(hin_words)-2]\n",
        "  return hin_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JH9LH-Whmq3",
        "outputId": "92a3ae92-b5ee-4549-da4d-bf26ab6b7b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "i=0\n",
        "res=[]\n",
        "act=[]\n",
        "for sentence in input_tensor_val:\n",
        "  convert(inp_lang, input_tensor_val[i])\n",
        "  hindi_w=translate(sentence)\n",
        "  act=convert2(targ_lang, target_tensor_val[i])\n",
        "  res=hindi_w.split()\n",
        "  res=res[0:len(res)-2]\n",
        "  print(\"ACTUAL\")\n",
        "  print(act)\n",
        "  print(\"TRANSLATED\")\n",
        "  print(res)\n",
        "  print(\"BLEU SCORE\")\n",
        "  print('Cumulative 1-gram: %f' % sentence_bleu([act], res, weights=(1, 0, 0, 0)))\n",
        "  print('Cumulative 2-gram: %f' % sentence_bleu([act], res, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('Cumulative 3-gram: %f' % sentence_bleu([act], res, weights=(0.33, 0.33, 0.33, 0)))\n",
        "  print('Cumulative 4-gram: %f' % sentence_bleu([act], res, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "  print(\"===========\")\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ----> <start>\n",
            "16 ----> it s\n",
            "21 ----> not\n",
            "6 ----> the\n",
            "1377 ----> th .\n",
            "2 ----> <end>\n",
            "Input: [   1   16   21    6 1377    2    0]\n",
            "Predicted translation: ये इतना ही नहीं है. <end> \n",
            "ACTUAL\n",
            "['आज', '25', 'नहीं']\n",
            "TRANSLATED\n",
            "['ये', 'इतना', 'ही', 'नहीं']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.250000\n",
            "Cumulative 2-gram: 0.500000\n",
            "Cumulative 3-gram: 0.632878\n",
            "Cumulative 4-gram: 0.707107\n",
            "===========\n",
            "1 ----> <start>\n",
            "384 ----> remember\n",
            "10 ----> what\n",
            "149 ----> mr .\n",
            "6741 ----> hallorann\n",
            "351 ----> said .\n",
            "2 ----> <end>\n",
            "Input: [   1  384   10  149 6741  351    2]\n",
            "Predicted translation: याद है कि क्या कहा कि क्या याद है. <end> \n",
            "ACTUAL\n",
            "['याद', 'करो', 'श्री', 'हैलोरन', 'ने', 'क्या']\n",
            "TRANSLATED\n",
            "['याद', 'है', 'कि', 'क्या', 'कहा', 'कि', 'क्या', 'याद']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.250000\n",
            "Cumulative 2-gram: 0.500000\n",
            "Cumulative 3-gram: 0.632878\n",
            "Cumulative 4-gram: 0.707107\n",
            "===========\n",
            "1 ----> <start>\n",
            "109 ----> thank\n",
            "78 ----> you ,\n",
            "149 ----> mr .\n",
            "655 ----> president .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input: [  1 109  78 149 655   2   0]\n",
            "Predicted translation: तुम्हारा क्रेडिट ठीक है, तो? <end> \n",
            "ACTUAL\n",
            "['मि.']\n",
            "TRANSLATED\n",
            "['तुम्हारा', 'क्रेडिट', 'ठीक', 'है,']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "===========\n",
            "1 ----> <start>\n",
            "9 ----> to\n",
            "89 ----> our\n",
            "3373 ----> potential\n",
            "1919 ----> success .\n",
            "2 ----> <end>\n",
            "Input: [   1    9   89 3373 1919    2    0]\n",
            "Predicted translation: उसकी लोग मर जाते हैं. <end> \n",
            "ACTUAL\n",
            "['हमारे', 'संभावित', 'सफलता', 'के']\n",
            "TRANSLATED\n",
            "['उसकी', 'लोग', 'मर', 'जाते']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "===========\n",
            "1 ----> <start>\n",
            "10 ----> what\n",
            "689 ----> brings\n",
            "4 ----> you\n",
            "9 ----> to\n",
            "7839 ----> chunghae ?\n",
            "2 ----> <end>\n",
            "Input: [   1   10  689    4    9 7839    2]\n",
            "Predicted translation: तुम्हें जाने के लिए क्या लाता है? <end> \n",
            "ACTUAL\n",
            "['आप', 'chunghae', 'करने', 'के', 'लिए', 'क्या', 'लाता']\n",
            "TRANSLATED\n",
            "['तुम्हें', 'जाने', 'के', 'लिए', 'क्या', 'लाता']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.564321\n",
            "Cumulative 2-gram: 0.535362\n",
            "Cumulative 3-gram: 0.497688\n",
            "Cumulative 4-gram: 0.430125\n",
            "===========\n",
            "1 ----> <start>\n",
            "57 ----> he s\n",
            "6 ----> the\n",
            "94 ----> very\n",
            "7259 ----> healthy\n",
            "2448 ----> guy !\n",
            "2 ----> <end>\n",
            "Input: [   1   57    6   94 7259 2448    2]\n",
            "Predicted translation: यह आप बहुत व्यस्त थी। <end> \n",
            "ACTUAL\n",
            "['यह', 'वही', '\"बेहद', 'तंदुरुस्त\"', 'वाला', 'बंदा']\n",
            "TRANSLATED\n",
            "['यह', 'आप', 'बहुत', 'व्यस्त']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.151633\n",
            "Cumulative 2-gram: 0.303265\n",
            "Cumulative 3-gram: 0.383860\n",
            "Cumulative 4-gram: 0.428882\n",
            "===========\n",
            "1 ----> <start>\n",
            "89 ----> our\n",
            "6837 ----> families\n",
            "574 ----> wouldn t\n",
            "25 ----> have\n",
            "6838 ----> split .\n",
            "2 ----> <end>\n",
            "Input: [   1   89 6837  574   25 6838    2]\n",
            "Predicted translation: हमारा चाहे उसका समय नहीं करेंगे . <end> \n",
            "ACTUAL\n",
            "['हमारा', 'परिवार', 'कभी', 'न']\n",
            "TRANSLATED\n",
            "['हमारा', 'चाहे', 'उसका', 'समय', 'नहीं', 'करेंगे']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.166667\n",
            "Cumulative 2-gram: 0.408248\n",
            "Cumulative 3-gram: 0.553618\n",
            "Cumulative 4-gram: 0.638943\n",
            "===========\n",
            "1 ----> <start>\n",
            "10 ----> what\n",
            "378 ----> day\n",
            "8 ----> is\n",
            "100 ----> it ?\n",
            "2 ----> <end>\n",
            "Input: [  1  10 378   8 100   2   0]\n",
            "Predicted translation: क्या बात क्या है, ? <end> \n",
            "ACTUAL\n",
            "['आज', 'कौन-सा', 'दिन']\n",
            "TRANSLATED\n",
            "['क्या', 'बात', 'क्या', 'है,']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "===========\n",
            "1 ----> <start>\n",
            "127 ----> now\n",
            "49 ----> come\n",
            "129 ----> on ,\n",
            "2455 ----> doc .\n",
            "2 ----> <end>\n",
            "Input: [   1  127   49  129 2455    2    0]\n",
            "Predicted translation: अब दल, था. <end> \n",
            "ACTUAL\n",
            "['अब', 'आओ,']\n",
            "TRANSLATED\n",
            "['अब', 'दल,']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.500000\n",
            "Cumulative 2-gram: 0.707107\n",
            "Cumulative 3-gram: 0.795536\n",
            "Cumulative 4-gram: 0.840896\n",
            "===========\n",
            "1 ----> <start>\n",
            "61 ----> we re\n",
            "191 ----> coming\n",
            "104 ----> right\n",
            "116 ----> now .\n",
            "2 ----> <end>\n",
            "Input: [  1  61 191 104 116   2   0]\n",
            "Predicted translation: हम अभी ठीक कर रहे हैं. <end> \n",
            "ACTUAL\n",
            "['हम', 'बस', 'अभी', 'आ', 'रहे']\n",
            "TRANSLATED\n",
            "['हम', 'अभी', 'ठीक', 'कर', 'रहे']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.600000\n",
            "Cumulative 2-gram: 0.774597\n",
            "Cumulative 3-gram: 0.844870\n",
            "Cumulative 4-gram: 0.880112\n",
            "===========\n",
            "1 ----> <start>\n",
            "61 ----> we re\n",
            "21 ----> not\n",
            "1117 ----> pulling\n",
            "494 ----> out ,\n",
            "2050 ----> alphonse .\n",
            "2 ----> <end>\n",
            "Input: [   1   61   21 1117  494 2050    2]\n",
            "Predicted translation: हम उस समय नहीं हम रहे हैं! <end> \n",
            "ACTUAL\n",
            "['हम', 'अल्फोंस', 'बाहर', 'खींच', 'नहीं']\n",
            "TRANSLATED\n",
            "['हम', 'उस', 'समय', 'नहीं', 'हम', 'रहे']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.577350\n",
            "Cumulative 3-gram: 0.695905\n",
            "Cumulative 4-gram: 0.759836\n",
            "===========\n",
            "1 ----> <start>\n",
            "3 ----> \n",
            "40 ----> no ,\n",
            "1572 ----> horse\n",
            "477 ----> shit .\n",
            "2 ----> <end>\n",
            "Input: [   1    3   40 1572  477    2    0]\n",
            "Predicted translation: - नहीं, किस तरह से गिर गया... <end> \n",
            "ACTUAL\n",
            "['-', 'नहीं,', 'घोड़े', 'की']\n",
            "TRANSLATED\n",
            "['-', 'नहीं,', 'किस', 'तरह', 'से', 'गिर']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.333333\n",
            "Cumulative 2-gram: 0.258199\n",
            "Cumulative 3-gram: 0.409157\n",
            "Cumulative 4-gram: 0.508133\n",
            "===========\n",
            "1 ----> <start>\n",
            "13 ----> we\n",
            "38 ----> can\n",
            "1172 ----> end\n",
            "794 ----> this !\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input: [   1   13   38 1172  794    2    0]\n",
            "Predicted translation: हम कर सकते हैं मैं कर सकते हैं मैं कर सकते हैं. <end> \n",
            "ACTUAL\n",
            "['हम', 'इस', 'समाप्त', 'कर', 'सकते']\n",
            "TRANSLATED\n",
            "['हम', 'कर', 'सकते', 'हैं', 'मैं', 'कर', 'सकते', 'हैं', 'मैं', 'कर', 'सकते']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.272727\n",
            "Cumulative 2-gram: 0.165145\n",
            "Cumulative 3-gram: 0.304642\n",
            "Cumulative 4-gram: 0.406380\n",
            "===========\n",
            "1 ----> <start>\n",
            "96 ----> tell\n",
            "91 ----> her\n",
            "86 ----> from\n",
            "4669 ----> toljan .\n",
            "2 ----> <end>\n",
            "Input: [   1   96   91   86 4669    2    0]\n",
            "Predicted translation: इसे बेहतर खो दिया? <end> \n",
            "ACTUAL\n",
            "['टॉल्जन', 'की', 'तरफ', 'से', 'उसे']\n",
            "TRANSLATED\n",
            "['इसे', 'बेहतर', 'खो']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "===========\n",
            "1 ----> <start>\n",
            "57 ----> he s\n",
            "7 ----> a\n",
            "814 ----> grown\n",
            "179 ----> man .\n",
            "2 ----> <end>\n",
            "Input: [  1  57   7 814 179   2   0]\n",
            "Predicted translation: वह एक बड़ा है. <end> \n",
            "ACTUAL\n",
            "['वह', 'एक', 'बड़ा', 'आदमी']\n",
            "TRANSLATED\n",
            "['वह', 'एक', 'बड़ा']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.716531\n",
            "Cumulative 2-gram: 0.716531\n",
            "Cumulative 3-gram: 0.716531\n",
            "Cumulative 4-gram: 0.716531\n",
            "===========\n",
            "1 ----> <start>\n",
            "20 ----> this\n",
            "8 ----> is\n",
            "140 ----> too\n",
            "1310 ----> much ,\n",
            "6867 ----> mister .\n",
            "2 ----> <end>\n",
            "Input: [   1   20    8  140 1310 6867    2]\n",
            "Predicted translation: यह तो हमारा कोलोराडो लाउंज है. <end> \n",
            "ACTUAL\n",
            "['यह', 'बहुत', 'ज्यादा', 'है,', 'मिस्टर']\n",
            "TRANSLATED\n",
            "['यह', 'तो', 'हमारा', 'कोलोराडो', 'लाउंज']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.200000\n",
            "Cumulative 2-gram: 0.447214\n",
            "Cumulative 3-gram: 0.587949\n",
            "Cumulative 4-gram: 0.668740\n",
            "===========\n",
            "1 ----> <start>\n",
            "226 ----> listen\n",
            "9 ----> to\n",
            "126 ----> me ,\n",
            "1002 ----> dear .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input: [   1  226    9  126 1002    2    0]\n",
            "Predicted translation: मुझे नहीं है. <end> \n",
            "ACTUAL\n",
            "['मेरी', 'बात', 'सुनो,']\n",
            "TRANSLATED\n",
            "['मुझे', 'नहीं']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.000000\n",
            "Cumulative 2-gram: 0.000000\n",
            "Cumulative 3-gram: 0.000000\n",
            "Cumulative 4-gram: 0.000000\n",
            "===========\n",
            "1 ----> <start>\n",
            "80 ----> his\n",
            "1214 ----> passport\n",
            "17 ----> and\n",
            "7876 ----> application\n",
            "2411 ----> form\n",
            "2 ----> <end>\n",
            "Input: [   1   80 1214   17 7876 2411    2]\n",
            "Predicted translation: मेरी लड़की ही लो और तो? <end> \n",
            "ACTUAL\n",
            "['उसका', 'पासपोर्ट', 'और', 'आवेदन']\n",
            "TRANSLATED\n",
            "['मेरी', 'लड़की', 'ही', 'लो', 'और']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.200000\n",
            "Cumulative 2-gram: 0.447214\n",
            "Cumulative 3-gram: 0.587949\n",
            "Cumulative 4-gram: 0.668740\n",
            "===========\n",
            "1 ----> <start>\n",
            "39 ----> that s\n",
            "3545 ----> anything ,\n",
            "3546 ----> basically ,\n",
            "1991 ----> standard .\n",
            "2 ----> <end>\n",
            "Input: [   1   39 3545 3546 1991    2    0]\n",
            "Predicted translation: यह हमारी मानक जाता है। <end> \n",
            "ACTUAL\n",
            "['यह', 'सामान्य', 'बात']\n",
            "TRANSLATED\n",
            "['यह', 'हमारी', 'मानक', 'जाता']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.250000\n",
            "Cumulative 2-gram: 0.500000\n",
            "Cumulative 3-gram: 0.632878\n",
            "Cumulative 4-gram: 0.707107\n",
            "===========\n",
            "1 ----> <start>\n",
            "13 ----> we\n",
            "151 ----> must\n",
            "48 ----> go\n",
            "129 ----> on ,\n",
            "5978 ----> ptolemy .\n",
            "2 ----> <end>\n",
            "Input: [   1   13  151   48  129 5978    2]\n",
            "Predicted translation: हम महान हो जाना चाहिए। <end> \n",
            "ACTUAL\n",
            "['हम', 'टॉलेमी', 'पर', 'जाना']\n",
            "TRANSLATED\n",
            "['हम', 'महान', 'हो', 'जाना']\n",
            "BLEU SCORE\n",
            "Cumulative 1-gram: 0.500000\n",
            "Cumulative 2-gram: 0.707107\n",
            "Cumulative 3-gram: 0.795536\n",
            "Cumulative 4-gram: 0.840896\n",
            "===========\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}